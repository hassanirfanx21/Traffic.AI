{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa0a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Ensure we are in the project root\n",
    "if os.path.basename(os.getcwd()) == \"notebooks_clean\":\n",
    "    os.chdir(\"..\")\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# Create models directory if not exists\n",
    "os.makedirs(\"models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569f1bd4",
   "metadata": {},
   "source": [
    "## **1. Load Data & Define Target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba715c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/suro_dataset_final.csv\")\n",
    "\n",
    "# Define Congestion Classes (Quantile-based)\n",
    "df['traffic_sum'] = df['inbound'] + df['outbound']\n",
    "q1 = df['traffic_sum'].quantile(0.33)\n",
    "q2 = df['traffic_sum'].quantile(0.66)\n",
    "\n",
    "def congestion_label(x):\n",
    "    if x <= q1: return 0   # Low\n",
    "    elif x <= q2: return 1 # Moderate\n",
    "    else: return 2         # High\n",
    "\n",
    "df['congestion_class'] = df['traffic_sum'].apply(congestion_label)\n",
    "\n",
    "# Features for Modeling\n",
    "features = [\n",
    "    \"latitude\", \"longitude\", \"hour\", \"day\", \n",
    "    \"temperature_2m\", \"precipitation\", \"rain\", \"wind_speed_10m\",\n",
    "    \"incident_flag\"\n",
    "]\n",
    "\n",
    "X = df[features]\n",
    "y = df['congestion_class']\n",
    "\n",
    "# Time-based Split (80/20)\n",
    "split_idx = int(0.8 * len(df))\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training Samples: {len(X_train)}, Test Samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5530ce78",
   "metadata": {},
   "source": [
    "## **2. Train Hierarchical Congestion Model**\n",
    "**Strategy:**\n",
    "- **Stage 1:** Binary classifier to find \"Moderate\" (Class 1) vs. Rest.\n",
    "- **Stage 2:** Binary classifier to find \"Low\" (0) vs. \"High\" (2) (trained only on non-moderate data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6caa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stage 1: Moderate vs. Rest ---\n",
    "print(\"Training Stage 1 (Moderate Specialist)...\")\n",
    "y_train_s1 = (y_train == 1).astype(int)\n",
    "clf_stage1 = HistGradientBoostingClassifier(\n",
    "    max_depth=20, learning_rate=0.15, max_iter=400, \n",
    "    class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "clf_stage1.fit(X_train, y_train_s1)\n",
    "\n",
    "# --- Stage 2: Low vs. High ---\n",
    "print(\"Training Stage 2 (Extreme Generalist)...\")\n",
    "mask_extreme = y_train != 1\n",
    "X_train_s2 = X_train[mask_extreme]\n",
    "y_train_s2 = y_train[mask_extreme]\n",
    "\n",
    "clf_stage2 = HistGradientBoostingClassifier(\n",
    "    max_depth=25, learning_rate=0.2, max_iter=450, \n",
    "    class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "clf_stage2.fit(X_train_s2, y_train_s2)\n",
    "\n",
    "# --- Evaluation ---\n",
    "print(\"Evaluating Hierarchical Model...\")\n",
    "probs_s1 = clf_stage1.predict_proba(X_test)[:, 1]\n",
    "preds_s2 = clf_stage2.predict(X_test)\n",
    "\n",
    "# Threshold tuned to 0.54 for optimal recall of Moderate class\n",
    "final_preds = np.where(probs_s1 > 0.54, 1, preds_s2)\n",
    "\n",
    "print(classification_report(y_test, final_preds))\n",
    "\n",
    "# Save Models\n",
    "joblib.dump(clf_stage1, 'models/congestion_stage1.pkl')\n",
    "joblib.dump(clf_stage2, 'models/congestion_stage2.pkl')\n",
    "print(\"Congestion models saved to models/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8a1eec",
   "metadata": {},
   "source": [
    "## **3. Train Incident Prediction Model**\n",
    "Predict the likelihood of an incident based on location, time, and weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380d9a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target: incident_flag\n",
    "y_inc = df['incident_flag']\n",
    "X_inc = df.drop(['congestion_class', 'incident_flag', 'traffic_sum', 'inbound', 'outbound'], axis=1, errors='ignore')\n",
    "\n",
    "# Stratified Split (Incidents are rare)\n",
    "X_train_inc, X_test_inc, y_train_inc, y_test_inc = train_test_split(\n",
    "    X_inc, y_inc, test_size=0.2, random_state=42, stratify=y_inc\n",
    ")\n",
    "\n",
    "print(\"Training Incident Model...\")\n",
    "model_inc = HistGradientBoostingClassifier(\n",
    "    max_depth=20, learning_rate=0.15, max_iter=350, \n",
    "    class_weight=\"balanced\", random_state=42\n",
    ")\n",
    "model_inc.fit(X_train_inc, y_train_inc)\n",
    "\n",
    "# Evaluation\n",
    "y_pred_inc = model_inc.predict(X_test_inc)\n",
    "print(classification_report(y_test_inc, y_pred_inc))\n",
    "\n",
    "# Save Model\n",
    "joblib.dump(model_inc, 'models/incident_model.pkl')\n",
    "print(\"Incident model saved to models/incident_model.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
