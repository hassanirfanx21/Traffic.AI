{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb0fc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import os\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import concurrent.futures\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Ensure we are in the project root\n",
    "if os.path.basename(os.getcwd()) == \"notebooks_clean\":\n",
    "    os.chdir(\"..\")\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31e4d7f",
   "metadata": {},
   "source": [
    "## **1. Load & Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca8b381",
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_df = pd.read_csv('datasets/traffic_dataset.csv')\n",
    "incident_df = pd.read_csv('datasets/incidents.csv')\n",
    "graph_df = pd.read_csv('datasets/graph_dataset.csv')\n",
    "\n",
    "# Clean Graph Data (Remove suffixes, average lat/long)\n",
    "graph_df['camera_name'] = graph_df['camera_name'].str.replace(r'-(inbound|outbound)$', '', regex=True)\n",
    "graph_merged = graph_df.groupby('camera_name').agg({\n",
    "    'latitude': 'mean',\n",
    "    'longitude': 'mean',\n",
    "    'sensor_id': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "# Merge Traffic with Graph\n",
    "suro_df = pd.merge(traffic_df, graph_merged, on='camera_name', how='left')\n",
    "\n",
    "# Parse Timestamps\n",
    "suro_df['timestamp'] = suro_df['timestamp'].apply(ast.literal_eval)\n",
    "suro_df['hour'] = suro_df['timestamp'].apply(lambda x: x[0])\n",
    "suro_df['minute'] = suro_df['timestamp'].apply(lambda x: x[1])\n",
    "\n",
    "# Create Datetime Column\n",
    "suro_df['date'] = suro_df['img_jpg'].apply(lambda x: \"-\".join(x.split(\"-\")[:3]))\n",
    "suro_df['datetime'] = pd.to_datetime(\n",
    "    suro_df['date'] + ' ' + \n",
    "    suro_df['hour'].astype(str) + ':' + \n",
    "    suro_df['minute'].astype(str) + ':00'\n",
    ")\n",
    "\n",
    "print(f\"Merged Data Shape: {suro_df.shape}\")\n",
    "suro_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff345feb",
   "metadata": {},
   "source": [
    "## **2. Merge Incidents (Time-Based)**\n",
    "Map incidents to traffic data using `merge_asof` to find incidents that occurred shortly before the traffic recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1400166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Incident Data\n",
    "incident_df['affected_sensors'] = incident_df['affected_sensors'].apply(ast.literal_eval)\n",
    "incident_exploded = incident_df.explode('affected_sensors').rename(columns={'affected_sensors': 'sensor_id'})\n",
    "incident_exploded['datetime'] = pd.to_datetime(incident_exploded['time'])\n",
    "incident_exploded['sensor_id'] = incident_exploded['sensor_id'].astype(int)\n",
    "\n",
    "# Clean for Merge\n",
    "suro_clean = suro_df.dropna(subset=['sensor_id', 'datetime']).copy()\n",
    "suro_clean['sensor_id'] = suro_clean['sensor_id'].astype(int)\n",
    "suro_sorted = suro_clean.sort_values(['sensor_id', 'datetime']).reset_index(drop=True)\n",
    "\n",
    "inc_clean = incident_exploded.dropna(subset=['sensor_id', 'datetime']).copy()\n",
    "inc_sorted = inc_clean.sort_values(['sensor_id', 'datetime']).reset_index(drop=True)\n",
    "inc_cols = inc_sorted[['sensor_id', 'datetime', 'incident_id']]\n",
    "\n",
    "# Execute Merge (Chunked by Sensor)\n",
    "result_chunks = []\n",
    "unique_sensors = sorted(suro_sorted['sensor_id'].unique())\n",
    "\n",
    "print(\"Merging incidents...\")\n",
    "for sensor_id in unique_sensors:\n",
    "    s_chunk = suro_sorted[suro_sorted['sensor_id'] == sensor_id]\n",
    "    i_chunk = inc_cols[inc_cols['sensor_id'] == sensor_id]\n",
    "    \n",
    "    if len(i_chunk) == 0:\n",
    "        s_chunk['incident_id'] = None\n",
    "        result_chunks.append(s_chunk)\n",
    "    else:\n",
    "        merged = pd.merge_asof(\n",
    "            s_chunk,\n",
    "            i_chunk,\n",
    "            on='datetime',\n",
    "            direction='backward',\n",
    "            tolerance=pd.Timedelta('10min')\n",
    "        )\n",
    "        result_chunks.append(merged)\n",
    "\n",
    "suro_final = pd.concat(result_chunks, ignore_index=True)\n",
    "suro_final['incident_flag'] = suro_final['incident_id'].notna().astype(int)\n",
    "suro_final.drop(columns=['incident_id'], inplace=True)\n",
    "\n",
    "print(f\"Incidents mapped. Flag count: {suro_final['incident_flag'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5bd52f",
   "metadata": {},
   "source": [
    "## **3. Enrich with Weather Data (Open-Meteo API)**\n",
    "Robust fetching with retries, caching, and threading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c070d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_FILE = 'datasets/weather_checkpoint.csv'\n",
    "MAX_WORKERS = 2\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "# Prepare Request Keys\n",
    "suro_final['api_date'] = pd.to_datetime(suro_final['date']).dt.strftime('%Y-%m-%d')\n",
    "unique_requests = suro_final[['camera_name', 'latitude', 'longitude', 'api_date']].drop_duplicates()\n",
    "unique_requests['key'] = unique_requests['camera_name'] + '_' + unique_requests['api_date']\n",
    "\n",
    "# Load Cache\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    try:\n",
    "        processed = pd.read_csv(CACHE_FILE, usecols=['camera_name', 'api_date'])\n",
    "        processed_keys = set(processed['camera_name'] + '_' + processed['api_date'])\n",
    "        print(f\"Loaded {len(processed_keys)} cached weather records.\")\n",
    "    except:\n",
    "        processed_keys = set()\n",
    "else:\n",
    "    processed_keys = set()\n",
    "    pd.DataFrame(columns=['camera_name', 'api_date', 'hour', 'temperature_2m', \n",
    "                          'precipitation', 'rain', 'snowfall', 'wind_speed_10m']).to_csv(CACHE_FILE, index=False)\n",
    "\n",
    "pending = unique_requests[~unique_requests['key'].isin(processed_keys)]\n",
    "print(f\"Pending requests: {len(pending)}\")\n",
    "\n",
    "# Setup Session\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "def fetch_weather(row):\n",
    "    try:\n",
    "        time.sleep(random.uniform(0.5, 1.5))\n",
    "        params = {\n",
    "            'latitude': row['latitude'],\n",
    "            'longitude': row['longitude'],\n",
    "            'start_date': row['api_date'],\n",
    "            'end_date': row['api_date'],\n",
    "            'hourly': 'temperature_2m,precipitation,rain,snowfall,wind_speed_10m',\n",
    "            'timezone': 'UTC'\n",
    "        }\n",
    "        resp = session.get(\"https://archive-api.open-meteo.com/v1/archive\", params=params, timeout=30)\n",
    "        if resp.status_code == 200:\n",
    "            data = resp.json()\n",
    "            hourly = data.get('hourly', {})\n",
    "            return [{\n",
    "                'camera_name': row['camera_name'],\n",
    "                'api_date': row['api_date'],\n",
    "                'hour': h,\n",
    "                'temperature_2m': hourly['temperature_2m'][h],\n",
    "                'precipitation': hourly['precipitation'][h],\n",
    "                'rain': hourly['rain'][h],\n",
    "                'snowfall': hourly['snowfall'][h],\n",
    "                'wind_speed_10m': hourly['wind_speed_10m'][h]\n",
    "            } for h in range(24)]\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {row['camera_name']}: {e}\")\n",
    "    return []\n",
    "\n",
    "# Execute\n",
    "if len(pending) > 0:\n",
    "    buffer = []\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(fetch_weather, row): row for _, row in pending.iterrows()}\n",
    "        for i, future in enumerate(concurrent.futures.as_completed(futures)):\n",
    "            res = future.result()\n",
    "            if res: buffer.extend(res)\n",
    "            if len(buffer) > 0 and (i % BATCH_SIZE == 0 or i == len(pending)-1):\n",
    "                pd.DataFrame(buffer).to_csv(CACHE_FILE, mode='a', header=False, index=False)\n",
    "                buffer = []\n",
    "                print(f\"Saved progress: {i}/{len(pending)}\")\n",
    "\n",
    "# Merge Weather\n",
    "weather_data = pd.read_csv(CACHE_FILE)\n",
    "weather_data.drop_duplicates(subset=['camera_name', 'api_date', 'hour'], inplace=True)\n",
    "suro_final = pd.merge(suro_final, weather_data, on=['camera_name', 'api_date', 'hour'], how='left')\n",
    "suro_final.drop(columns=['api_date'], inplace=True)\n",
    "\n",
    "suro_final.to_csv(\"datasets/suro_dataset_final.csv\", index=False)\n",
    "print(\"Final dataset saved to datasets/suro_dataset_final.csv\")\n",
    "suro_final.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
